<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Tech Garage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="#" class="logo">Mathematics and Computer Science Blogging</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="techgarage.html">Tech Garage</a></li>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									
								</header>
								
								

								<article>
									<header>
										<h2><a href="#">A quick dive into the RegVGG re-parameterization</a></h2>
									</header>

									

									<p>In this blog post, we focus on the mathematical details of the "structural re-parameterization" in a Rep-VGG block (see section 3.3 and fig. 4 of the <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Ding_RepVGG_Making_VGG-Style_ConvNets_Great_Again_CVPR_2021_paper.html">paper</a>). This block transformation describes how to transform a RepVGG training-time block to a single RepVGG \(3\times3\) inference-time block.


									</p>
									<p>
										We take the same notations from the paper. We have:
										<ul>
											<li>
												Let \(W^{(3)} \in \mathbb{R}^{C_2 \times C_1 \times 3 \times 3}\) to denote the kernel of a \(3 \times 3\) convolution layer with \(C_1\) input channels and \(C_2\) output channels.
											</li> 
											<li>
												Let the definition of \(W^{(1)}\) be analogous of \(W^{(3)}\)
											</li>
										</ul>
									</p>
									<p>
										In figure 4, the authors display the architecture of a Rep-VGG block at training-time and inference-time along with an illustration of the re-parameterization equations. At training-time, we have the output expressed as:
									</p>
									<p>\(y = y_1 + y_2 + y_3 = BN^{(3)}(Conv^{(3)}(x)) + BN^{(1)}(Conv^{(1)}(x)) + BN^{(0)}(x)\)</p>
									<p>\(y = BN^{(3)}(W^{(3)}x + b^{(3)}) + BN^{(1)}(W^{(1)}x + b^{(1)}) + BN^{(0)}(x)\)</p>
									<p>\(y = BN^{(3)}(W^{(3)}x + b^{(3)}) + BN^{(1)}(W^{(1)}x + b^{(1)}) + BN^{(0)}(x)\)</p>
									<p>\(y = \gamma^{(3)}(W^{(3)}x + b^{(3)}) + \beta^{(3)} + \gamma^{(1)}(W^{(1)}x + \beta^{(1)}) + \gamma^{(0)}x + \beta^{(0)}\)</p>

									<p>
										The expression \(\gamma^{(0)}x + \beta^{(0)}\) can be rewritten as a \(1\times 1\) convolution with identity kernel, padding \(0\) and stride \(1\).
									</p>
									<p>
										In a second time, let's write the re-parameterization equation of a \(1 \times 1\) convolution into a \(3 \times 3\) convolution. Let us note first that the convolution of kernel \(W^{(3)}\) has padding \(1\) and stride \(1\) (such that the input and output resolution of the features remain the same). Similarly, the \(1 \times 1\) convolution has padding \(0\). Obviously, the makes the resulting sum \(y = y_1 + y_2 + y_3\) of the mutli branch block possible.
									</p>
									<p>
										In these conditions, a \(1 \times 1\) convolution kernel \(W_a^{(1)}\) can simply be padded with zeros to a corresponding convolution kernel \(W_a^{(3)}\). As a result the new \(3 \times 3\) convolution will have padding \(1\) instead of \(0\). In this case, we have the following equality: \(W_a^{(1)}x = W_a^{(3)}x\).
									</p>
									<p>
										Now, when we rewrite the convolution sum, the kernel part of the summation can be added which is equivalent to sum the two kernels:
									</p>
									<!-- <p>
										\(\sum\limits_{a, b} \sum\limits_{i, j} F(a + i, b + j) \times W^{(3)}(i, j) + \sum\limits_{a, b}\)
									</p> -->
									<p>
										\(\sum\limits_{a, b} \sum\limits_{i, j} F(a + i, b + j) \times W^{(3)}(i, j) + \sum\limits_{a, b} \sum\limits_{i, j} F(a + i, b + j) \times W_a^{(3)}(i, j) = \sum\limits_{a, b} \sum\limits_{i, j} F(a + i, b + j) \times (W^{(3)}(i, j) + W_a^{(3)}(i, j))\)
									</p>
									<p>
										In other words, the new kernel \(W'^{(3)} = W^{(3)} + W_a^{(3)}\) can be viewed as \(W'^{(3)}(i, j) = W^{(3)}(i, j)\) if $\((i, j) \neq (1, 1)\) and \(W'^{(3)}(1, 1) = W^{(3)}(1, 1) + W_a^{(3)}(1, 1). \ \ \ \diamond\)
									</p>
								</article>
							</section>

					</div>


				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; plefevre</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>